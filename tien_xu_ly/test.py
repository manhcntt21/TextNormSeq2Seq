
# # coding=utf-8

# import json
# import  core_nlp.tokenization.crf_tokenizer 

# # data = []
# # with open('data_.jsonl', 'r') as json_data:
# #     for f in json_data:
# #     	# tmp = f['raw'].split()
# #     	data.append(json.loads(f))    
# # # i = 0

# # for d in data:
# # 	d['raw'] = d['raw'].split()
# # 	d['original'] = d['original'].split()
# # 	# print(d)
# # print(data[1])
